# 说明

现在想把这个工作改投到期刊上，需要做一些改进和扩容的工作。改进和扩容的工作可以分成以下三个方向进行：

~~1）讨论加residual block的意义和重要性。~~
	residual block的作用没有那么明显，在`dense`采样下`model`的性能已经优于之前最好的结果了。尤其是在`mAP`上得到了差不多`0.5%`的提升。

~~2）讨论采用之前那个训练方法的重要性。~~
	目前的训练方法是最优的。因为我们的目的是，让高层特征能发现低层所没有发现的特征。而通过目前的方式，能相对简单的做到这一点。从输出的判断概率来看，也确实高层特征判断能力要强于低层特征。

3）讨论采用不同的reference的重要性。
*	首先我尝试了用全部reference的参考生成`TRA module`，发现效果确实是目前最优的。尤其是，在`dense`采样下`mAP`达到了`86.9%`差不多是目前最高的结果了。（但还可以探索一下用global reference 的结果。）

4）讨论Spatial-wise information 和 channel-wise information 结合的更优方法。

*	~~修改了一个我之前认为有点不太合理的`channel-wise attention socre generation method`。具体就是将 $ s_1 = FC(f_1^c, f_2^c); s_2 = FC(f_2^c, f_1^c)$改成了$[s_1, s_2] = FC(f_1^c,f^c_2)$。但是效果没有之前的好，哪怕用`dense`采样，`mAP`也能达到`86.7%`，`Rank1`也只有`90.9%`。这样的结果，和之前的都没法比。~~但是直观上，这种方法应该才是更有效的。为什么没有起作用呢？
*	 ~~~这个修改实际上是work了的。我们避免了中间使用这么多的全连接层来增加训练的难度。仅用了两层。详细的实现过程见`exp 6`。<font color='red'>这个实验最后在`Rank`上取得了目前几乎最优的结果。</font>~~~
*	 将`exp6`中的合并改成了相加，想达到保证性能的前提下，降低运算量。但效果不佳。
*	 

5）讨论TRA和SRA更优的结合方式。

6）讨论降低运算量的方法。

7）讨论层与层之间的`linkage`，用KL散度。用后几层的输出去教前几层的输出，然后测试的时候让第一层就能发挥接近第三层的结果。

8）讨论能否将金字塔的结构嵌入`resnet50`中，以达到降低运算量的目的。

9）讨论将`TRA`和`SRA`合并起来会怎么样。

10）讨论将一个`global reference`参与到`SRA`和`TRA`运算的方式。

11）如果降低了运算量，我们能不能讨论三个三个合并？四个四个合并？等等。。（讨论中的一种。）

12）准备在`LS-VID`中进行实验，包括消融实验，可视化以及检索的结果。

13）在另外两个小库上进行检索结果的对比。（有可能比baseline不好！）

14）在论文说明的时候，可以说明一下我们模型的高`baseline`设置。

15）讨论一下论文的训练差异。


# exp 1

虽然，当前的试验结果是最优的。但是，当去掉了residual block Rank1性能差不多下滑了1%。不知道是模型的波动还是什么其他的问题。
但是，在之前的实验中，证明了不用residual block 也是能达到比较好的性能的。可以对比一下，这两者的差别。


# exp 2

试图通过调整三层训练的权重来让这三层特征向量得到不同程度的训练。但是，发现修改之后并没有很好的效果。输出三层特征向量对应的全连接层的概率输出，发现第三层的结果普遍要高于前两层。
这只能说明，确实将三层特征取平均，能更好发挥模型特征互补的效果。感觉还是有些不太完美。
为什么，第三层的输出值这么小，还能做到这么好的互补效果呢？第三层找到的到底是什么样的特征？


# exp 3

因为发现之前，前三层的输出特征向量都非常的小，想着这个可能是没有将原来的特征向量加上去的原因。在本实验中，在原来 $\alpha \times F_i + \theta \times F_j$的基础上，将运算公式改成了 $\alpha \times F_i + \theta \times F_j + F_i + F_j$。相比原来，模型的性能在`rank1`上有了差不多 $0.2\%$ 的提升。

这至少说明这么改进应该是有作用的。可以测试一下，此时模型的输出值是一个什么情况。看一下值小的问题是不是确实别解决了，还是说值是没有变化的。

通过对比发现，模型对应的特征值输出确实有了变化，略微变大了一些。

# exp 4

基于`exp3`我继续尝试了下，在`SRA`和`TRA`都采用了这种`shortcut`的`setting`。但是，效果不佳。说明过度的`shortcut`有可能干扰模型的学习。

# exp 5

基于`exp3`，我们可以继续尝试一下采用不同`reference`的结果。
我发现用不降维的`all reference`能够获得很高的`mAP`。
但是在`rank1`的表现上没有这么的亮眼。

# exp 6

基于`exp5`使用全局`reference`，并且修改了`channel-wise attention mask`的生成方式，大体的实现方式为
$$
f^c_{i,j} = Cat[f^c_{i}, f^c_{j}]
$$
$$
[f^c_{i,1},f^c_{j,1}] = FC[f^c_{i,j}]
$$
$$
f^c_{i,2} = FC[f^c_{i,1}]
$$
$$
f^c_{j,2} = FC[f^c_{j,2}]
$$

最终实现了在`Rank1`达到了目前的最高性能，

|Model|mAP|Rank1|Rank5|
|:--:|:--:|:--:|:--:|
|Baseline|84.7|88.8|97.0|
|ICCV|85.8|91.5|97.0|
|exp 6|87.0|91.5|97.3|

但需要弄清楚的是，目前所作的修改还远达不到`30%`的效果。至少，不是在模型本质上做了什么不一样的修改。

（突然想到一个问题，如果是想加而不是合并，会怎么样？？用实验来验证一下。）

# exp-7

基于`exp-6`将原来的合并改成了相加，即将公式`(1)`中的合并改成了
$$
f^c_{i,j} = f^c_{i} + f^c_{j}
$$

最终得到的实验结果为，

|Model|mAP|Rank1|Rank5|
|:--:|:--:|:--:|:--:|
|Base|84.7|88.8|97.0|
|ICCV|85.8|91.5|97.0|
|exp6-1|86.9|91.6|97.1|
|exp6-2|87.0|91.5|97.3|
|exp7|86.8|91.2|97.4|
|DenseIL(ICCV)|87.0|90.8|97.1|

从这个结果上来看，`exp7`的改动没有原来的好，虽然增加了运算量，但是使模型的性能出现了下划。但这也比较好理解，我们需要考虑的是两个通道之间的联系，而不是这两个通道的共同信息。所以直接相加并不那么合理。但这就不可避免的增加了模型的复杂度。这个问题怎么解决？

# exp-8

基于 `exp-6`，将`global reference`加入`spatial attention`和`temporal attention`的生成。